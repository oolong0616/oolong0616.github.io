---
layout:     post  
title:      2、快速入门    
subtitle:   笔记：Spark 官方文档(更新至2.4.3)  
date:       2018-05-22   
author:     岑晨  
header-img: 
catalog: true  
tags:  

    - spark   
---

# 概述
在 Spark 2.0 之前，Spark 的主要编程接口是弹性分布式数据集（RDD）。在 Spark 2.0之后，RDD 被 Dataset 替换，它是像RDD 一样的strongly-typed（强类型），但是在引擎下更加优化。  

Spark 默认安全关闭，

> RDD、DataSet、DataFrame对比 
> - RDD: 弹性数据集,有简单类型数据构成
> - DataSet和DataFrame：DataSet和DataFrame有完全相同的成员函数，都支持Spark SQL，其最大的区别是Row的类型
>   - DataSet：强类型化的RDD，Python中不可以使用DataSet。
>   - DataFrame：由列组成的DataSet，等同于数据库或者R/Python 中的表。


> Application、SparkSession、SparkContext  
> ![Aaron Swartz](https://raw.githubusercontent.com/oolong0616/oolong0616.github.io/master/img/post-ksrm-ASSR.png)
>
> - Application:  用户编写的Spark应用程序，Driver 运行上述 Application 的 main() 函数，并且创建 SparkContext。
> - SparkSession: SparkSession封装了SparkConf、SparkContext 和SQLContext，Shell中`Spark` 是默认的SparkSession
> - SparkContext:整个应用的上下文，控制应用的生命周期,shell中SC是默认的SparkContext

# Spark Shell
##  命令位置(python)
```bash
$SPARK_HOME/bin/pyspark
```
##  命令详解（Python）   

```python
textFile = spark.read.text("file:///userDir/spark/spark-2.4.1-bin-hadoop2.6/README.md")
    # Spark配置SPARK_DIST_CLASSPATH后，默认从HDFS中找文件，如果本机需要加file://
    # 该函数将文件按照换行分割，存入多个Row
textFile.count() 
    # 105 ，textFile为一个DataFrame，这里统计的是行数
textFile.first() 
    # Row(value=u'# Apache Spark') ，DataFrame中每个子对象是一个Row对象，第一行
linesWithSpark = textFile.filter(textFile.value.contains("Spark"))
    # 过滤，返回所有包含"Spark"的行
    # DataFrame filter 返回类型是Row，Lambda表达式只能处理String等简单类型，所以，不可以用lambda表达式
    # filter 函数：返回textFile中符合条件的所有Row
textFile.filter(textFile.value.contains("Spark")).count() 
    # 合并Transform操作与Action操作，由于惰性运算，合并不合并无所谓  
 textFile.select(size(split(textFile.value, "\s+")).name("numWords")).agg(max(col("numWords"))).collect()
# 取文档中单词做多的行的单词数
# textFile.select:DataFrame.select()返回包含指定列或者expressions (Column)的信息
# split(textFile.value, "\s+")：\s+代表多个空白字符，将文档各行按照空白字符进行分割
# size(split(textFile.value, "\s+"):返回每行单词数量
# name("numWords"):将Column命名
# agg:即aggregate 聚合函数，由于Spark为并行计算，需要将各Worker中的计算结果进行聚合，
# col("numWords")：取出指定列名Column
wordCounts = textFile.select(explode(split(textFile.value, "\s+")).alias("word")).groupBy("word").count()
# 统计文档中各单词出现的次数
# explode：将一行数据分为多行

```

# 高速缓存  
热数据或者交互式分析是，可将数据集提取到群集范围的内存缓存中  
```python
.cache（）
```
# 独立程序发布（Python）

- 编写应用程序

  ```python
  """SimpleApp.py"""
  from pyspark.sql import SparkSession
  
  logFile = "file:///userDir/spark/spark-2.4.1-bin-hadoop2.6/README.md" 
  spark = SparkSession.builder.appName("SimpleApp").getOrCreate()
  # 创建SparkSession
  logData = spark.read.text(logFile).cache()
  
  numAs = logData.filter(logData.value.contains('a')).count()
  numBs = logData.filter(logData.value.contains('b')).count()
  
  print("Lines with a: %i, lines with b: %i" % (numAs, numBs))
  
  spark.stop()
  ```
  
- 执行应用程序

  ```bash
  YOUR_SPARK_HOME/bin/spark-submit  --master local[4]  /XXX/SimpleApp.py
  ```




> 参照文档  
				[ApacheCN](https://github.com/oolong0616/spark-doc-zh.git)     
				[Spark官方文档](http://spark.apache.org/docs/2.2.0/quick-start.html)      
