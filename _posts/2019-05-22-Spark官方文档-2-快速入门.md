---
layout:     post  
title:      Spark 官方文档（2.2.0）   
subtitle:   快速入门 
date:       2019-05-20  
author:     岑晨  
header-img: 
catalog: true  
tags:  
      - Spark  
---  
# 概述
在 Spark 2.0 之前，Spark 的主要编程接口是弹性分布式数据集（RDD）。在 Spark 2.0
之后，RDD 被 Dataset 替换，它是像RDD 一样的
strongly-typed（强类型），但是在引擎盖下更加优化。  
> RDD、DataSet、DataFrame对比 
> - RDD: 弹性数据集
> - DataSet:是一个优化之后的RDD。他的底层调用的还是RDD。只不过在执行之前，他有一个执行计划，优化器，他会对RDD进行优化然后再执行。DateSet比RDD的好处就是，他里面的数据类型都是强类型。而以前我们RDD里面默认装的都是String类型。没有具体指定是什么类型。如果用之前的我们切分完数据之后如果想和Int类型进行比较的话还需要使用toInt等操作进行转换。但是以后我们的DateSet就是强类型的，我们可以进行指定。并且一旦指定是什么类型的我们就可以用lambda表达式的方式对DataSet进行处理  
> - DataFrame(Python只有DataFrame):本质上是一个DataSet，在DataSet中是以被命名的列的形式组织的。也就是说在spark中DataFrame其实是DataSet中装的row他在概念上和我们关系型数据库中的表是类似的。我们之所以可以进行优化，是因为一旦我们知道了有哪些列，我们就可以只加载我们需要的列，而不用全部加载。
   
> Application、SparkSession、SparkContext、RDD  

![Aaron Swartz](https://raw.githubusercontent.com/oolong0616/oolong0616.github.io/master/img/post-ksrm-ASSR.png)
> - Application:  
> - SparkSession:
> - SparkContext:
> - RDD   

# Spark Shell
## 命令位置(python)
```
$SPARK_HOME/bin/pyspark
```   


> 参照文档  
    ApacheCN：https://github.com/oolong0616/spark-doc-zh.git  
    Spark官方：http://spark.apache.org/docs/2.2.0/    


 