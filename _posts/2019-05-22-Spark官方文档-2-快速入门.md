---
layout:     post  
title:      笔记：Spark 官方文档(2.2.0)    
subtitle:   2、快速入门  
date:       2018-05-20  
author:     岑晨  
header-img: 
catalog: true  
tags:  

    - spark   
    
    - 笔记
---

# 概述
在 Spark 2.0 之前，Spark 的主要编程接口是弹性分布式数据集（RDD）。在 Spark 2.0之后，RDD 被 Dataset 替换，它是像RDD 一样的strongly-typed（强类型），但是在引擎下更加优化。  
> RDD、DataSet、DataFrame对比 
> - RDD: 弹性数据集，不支持SparkSQL操作
> - DataSet和DataFrame：DataSet和DataFrame有完全相同的成员函数，都支持Spark SQL，其最大的区别是Row的类型
>   - DataSet：
>   - DataFrame：


> Application、SparkSession、SparkContext、RDD  
> ![Aaron Swartz](https://raw.githubusercontent.com/oolong0616/oolong0616.github.io/master/img/post-ksrm-ASSR.png)
>
> - Application:  用户编写的Spark应用程序，Driver 即运行上述 Application 的 main() 函数并且创建 SparkContext。
> - SparkSession: SparkSession封装了SparkConf、SparkContext 和SQLContext，Shell中`Spark` 是默认的SparkSession
> - SparkContext:整个应用的上下文，控制应用的生命周期,shell中SC是默认的SparkContex

# Spark Shell
##  命令位置(python)
```bash
$SPARK_HOME/bin/pyspark
```
##  命令详解（Python）   

```python
textFile = spark.read.text("file:///userDir/spark/spark-2.4.1-bin-hadoop2.6/README.md")
    # Spark配置SPARK_DIST_CLASSPATH后，默认从HDFS中找文件，如果本机需要加file://
    # 该函数将文件按照换行分割，存入多个Row
textFile.count() 
    # 105 ，textFile为一个DataFrame，这里统计的是行数
textFile.first() 
    # Row(value=u'# Apache Spark') ，DataFrame中每个子对象是一个Row对象，第一行
linesWithSpark = textFile.filter(textFile.value.contains("Spark"))
    # 过滤，返回所有包含"Spark"的行
    # DataFrame filter 返回类型是Row，Lambda表达式只能处理String等简单类型，所以，不可以用lambda表达式
textFile.filter(textFile.value.contains("Spark")).count() 
    # 合并Transform操作与Action操作，由于惰性运算，合并不合并无所谓   
```

# 高速缓存  
将数据集提取到群集范围的内存缓存中  
```python
.cache（）
```
# 独立程序发布（Python）

```bash
$SPARK_HOME/bin/spark-submit \--master local[4]  \ XXX.py
```



> 参照文档  
    ApacheCN：https://github.com/oolong0616/spark-doc-zh.git  
    Spark官方：[http://spark.apache.org/docs/2.2.0/quick-start.html](http://spark.apache.org/docs/2.2.0/quick-start.html)    


