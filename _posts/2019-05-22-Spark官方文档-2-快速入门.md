---
layout:     post  
title:      Spark 官方文档（2.2.0）   
subtitle:   快速入门 
date:       2019-05-20  
author:     岑晨  
header-img: 
catalog: true  
tags:  
      - Spark  
---  
# 概述
在 Spark 2.0 之前，Spark 的主要编程接口是弹性分布式数据集（RDD）。在 Spark 2.0
之后，RDD 被 Dataset 替换，它是像RDD 一样的
strongly-typed（强类型），但是在引擎盖下更加优化。  
> RDD、DataSet、DataFrame对比 
> - RDD: 弹性数据集
> - DataSet:是一个优化之后的RDD。他的底层调用的还是RDD。只不过在执行之前，他有一个执行计划，优化器，他会对RDD进行优化然后再执行。DateSet比RDD的好处就是，他里面的数据类型都是强类型。而以前我们RDD里面默认装的都是String类型。没有具体指定是什么类型。如果用之前的我们切分完数据之后如果想和Int类型进行比较的话还需要使用toInt等操作进行转换。但是以后我们的DateSet就是强类型的，我们可以进行指定。并且一旦指定是什么类型的我们就可以用lambda表达式的方式对DataSet进行处理  
> - DataFrame(Python只有DataFrame):本质上是一个DataSet，在DataSet中是以被命名的列的形式组织的。也就是说在spark中DataFrame其实是DataSet中装的row,他在概念上和我们关系型数据库中的表是类似的。我们之所以可以进行优化，是因为一旦我们知道了有哪些列，我们就可以只加载我们需要的列，而不用全部加载。
   
> Application、SparkSession、SparkContext、RDD  
> ![Aaron Swartz](https://raw.githubusercontent.com/oolong0616/oolong0616.github.io/master/img/post-ksrm-ASSR.png)
> - Application:  用户编写的Spark应用程序，Driver 即运行上述 Application 的 main() 函数并且创建 SparkContext。
> - SparkSession: SparkSession封装了SparkConf、SparkContext 和SQLContext，Shell中Spark 是默认的SparkSession
> - SparkContext:整个应用的上下文，控制应用的生命周期,shell中SC是默认的SparkContex

# Spark Shell
##  命令位置(python)
```
$SPARK_HOME/bin/pyspark
```      
##  命令详解（Python）   

```
textFile = spark.read.text("file:///userDir/spark/spark-2.4.1-bin-hadoop2.6/README.md")
    # Spark配置SPARK_DIST_CLASSPATH后，默认从HDFS中找文件，如果本机需要加file://
    # 该函数将文件按照换行分割，存入多个Row
textFile.count() 
    # 105 ，textFile为一个DataFrame
textFile.first() 
    # Row(value=u'# Apache Spark') ，DataFrame中每个子对象是一个Row对象
linesWithSpark = textFile.filter(textFile.value.contains("Spark"))
    # 过滤，返回所有包含"Spark"的行
textFile.filter(textFile.value.contains("Spark")).count() 
    # 合并Transform操作与Action操作，由于惰性运算，合并不合并无所谓   
    
```

> 参照文档  
    ApacheCN：https://github.com/oolong0616/spark-doc-zh.git  
    Spark官方：http://spark.apache.org/docs/2.2.0/    


 