---
layout:     post  
title:      5、Hive    
subtitle:   笔记：CDH大数据平台实战指南 
date:       2019-06-25  
author:     岑晨  
header-img: 
catalog: true  
tags:  
    - CDH 
---

# 概览

将类SQL语句转成MapReduce任务，并执行的数据仓库工具。

- Hive 是为大数据批量处理而生的： Hive 的出现解决了传统的关系型数据库（如 MySQL 、
  Oracle ）在大数据处理上的瓶颈，由于 Hive 是建立在 Hadoop 的分布式文件系统HDFS 之上的，通过 MapReduce 计算框架来执行用户提交的任务，因此可以支持很大规模的数据，此外 Hive 的可扩展性和 Hadoop 的可扩展性是一致的 。
-  Hive 执行延迟高： Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 H ive 执行延迟高的因素是 MapReduce 计算框架，由于 MapReduce
  本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。由于数据的访问延迟较高，决定了 Hive不适合实时数据查询， Hive 最佳使用场景是大数据集的批处理作业，
  比如网络日志分析。

# 体系架构

![image-20190815165213769](/MyDoc/05_WorkSpace/oolong0616.github.io/img/image-20190815165213769.png)

- 元数据：元数据存储在RMDB（默认Derby，建议Mysql）
- 数据：以文件形式存放在HDFS中，所以，不支持数据改写和添加，数据应在加载时已确定。
- 不支持事务：只有查询，要事务也的确没啥用。

#  数据模型

- 内部表和外部表：
  - 同：都包含存储数据和元数据，存储数据都通过HDFS文件形式存放，元数据存放映射关系，存储在RMDB中。
  - 异：内部表存储数据存放在指定位置（先建表在上传文件），外部表存放任意位置，Drop时内部表存储数据随之删除，外部表不删除。
- 分区和分桶
  - 同：都是通过将一个大文件按照指定字段分割为多个小文件解决文件读取问题
  - 异：分区在上传数据时制定数据存放区，分桶自动进行划分，分区会出现数据分布不均衡，分桶不会（Hash算法保证）。
  - 分区表：一个表可以有一个或多个分区，各分区以文件夹的形式单独存在表文件夹的目录下，避免 Hive Select 查询中扫描整个表内容。
  - 桶：桶是更为细粒度的数据范围划分，Hive 采用对指定列计算哈希 hash，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中，桶是以文件的形式存放在表的目录下，每一个桶对应一个文件，建议在抽样分析时采用桶。
- 索引：hive只有有限的索引功能，hive中没有普通关系型数据库中键的概念，但是还是可以对一些字段建立索引来加速某些操作的，一张表的索引数据在另一张表中，说到索引我们也可以理解为这是hive提供的优化功能。他可以减少MapReduce的输入数据量，因为在索引表中他把每个字段的索引和偏移量都计算出来，可以说查找的速度是很快了，尤其是大数据集。

# 实战

- 内部表操作

  ```bash
  > hive
  show databases;
  create database if not exists testDemo;
  create table visiters(last_name string,first_name string,arrival_time string,scheduled_time string,metting_location string,info_comment string) 
  row format delimited fields terminated by '\t';
  # row format delimited fields terminated by '\t'：文件以‘\t’分割
  # HDFS 查看文件
  > hdfs dfs -ls /user/hive/warehouse/testdemo.db
  # 0
  # 上传文件
  >  hdfs dfs -put /userDir/ftpDir/visiter.txt /user/hive/warehouse/testdemo.db/visiters
  > hdfs dfs -ls /user/hive/warehouse/testdemo.db/visiters
  # -rw-r--r--   3 hdfs hive        279 2019-08-16 16:55 /user/hive/warehouse/testdemo.db/visiters/visiter.txt
  # 查询数据
  select * from visiters;
  #注意：表名和文件名的区别，hive识别的是表名=HDFS路径文件夹名称，而非文件名。
  ```

- 外部表操作

  ```bash
  # 上传文件到HDFS
  > hdfs dfs -mkdir /user/hive/warehouse/hivedemo.db/test
  > hdfs dfs -put /userDir/ftpDir/visiter.txt /user/hive/warehouse/hivedemo.db/test
  # Hive 
  #	1、指定数据路径
  #	此方式要求：文件上传到内置位置，文件路径名=表名
  > hive
  use hivedemo;
  create external table test2(last_name string,first_name string,arrival_time string,scheduled_time string,metting_location string,info_comment string) 
  row format delimited fields terminated by '\t'
  location '/user/hive/warehouse/hivedemo.db/test/';
  # 2、是创建表时不指定数据目录 ，随后把加载
  create external table test2(last_name string,first_name string,arrival_time string,scheduled_time string,metting_location string,info_comment string) 
  row format delimited fields terminated by '\t';
  load data local inpath '/userDir/ftpDir/visiter.txt' overwrite into table test2;
  >  hdfs dfs -ls /user/hive/warehouse/hivedemo.db/
  # drwxrwxrwt   - root hive          0 2019-08-16 18:48 /user/hive/warehouse/hivedemo.db/test2
  # 注：导入后，该文件上传到HDFS，并放置到默认路径下
  ```

- 查看表属性

  ```bash
  desc formatted test2;
  ```
  
- 分区表操作

  ```bash
  #创建分区表 
  create  table test3(id string,name string) partitioned by (ds string)
  row format delimited fields terminated by '\t'
  line terminated by '\n'
  stored as textfile;
  # 加载分区表数据
  load data local inpath '/userDir/ftpDir/test1.txt' overwrite into table test3 partition (ds='2014');
  load data local inpath '/userDir/ftpDir/test1.txt' overwrite into table test3 partition (ds='2015');
  #HDFS验证  
  > hdfs dfs -ls /user/hive/warehouse/hivedemo.db/test3
  > # drwxrwxrwt   - root hive          0 2019-08-21 16:20 /user/hive/warehouse/hivedemo.db/test3/ds=2014
  > # drwxrwxrwt   - root hive          0 2019-08-21 16:25 /user/hive/warehouse/hivedemo.db/test3/ds=2015
  ```

- 分桶表操作

  ```bash
  # 配置Hive可以识别桶
  set hive.enforce.bucketiong=true;
  create table bucket_table1(id int) clustered by(id) into 8 buckets;
  # 分桶表不能直接加载数据
  create table bucket_table2(id int);
  load data local inpath '/userDir/ftpDir/1.txt' overwrite into table bucket_table2;
insert into bucket_table1 select * from bucket_table2;
  
  ```
  
  